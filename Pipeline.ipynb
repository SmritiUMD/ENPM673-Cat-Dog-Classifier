{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import model\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.image as mpimg\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = mpimg.imread('data/train/cat.0.png')\n",
    "img2 = mpimg.imread('data/train/cat.1000.png')\n",
    "img3 = mpimg.imread('data/train/dog.10000.png')\n",
    "img4 = mpimg.imread('data/train/dog.1001.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = plt.figure()\n",
    "a = fig.add_subplot(1, 2, 1)\n",
    "imgplot = plt.imshow(img1)\n",
    "a.set_title('img1')\n",
    "a = fig.add_subplot(1, 2, 2)\n",
    "imgplot = plt.imshow(img2)\n",
    "a.set_title('img2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure()\n",
    "a = fig1.add_subplot(1, 2, 1)\n",
    "imgplot = plt.imshow(img3)\n",
    "a.set_title('img3')\n",
    "a = fig1.add_subplot(1, 2, 2)\n",
    "imgplot = plt.imshow(img4)\n",
    "a.set_title('img4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CatandDog(Dataset):\n",
    "    \n",
    "\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.labels= pd.read_csv(csv_file)\n",
    "       \n",
    "      \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        image_name = os.path.join(self.root_dir,\n",
    "                                self.labels.iloc[idx, 0])\n",
    "        #print(image_name)\n",
    "\n",
    "        image = mpimg.imread(image_name)\n",
    "#         print(image.shape)\n",
    "        \n",
    "        #if image has an alpha color channel, get rid of it\n",
    "        if(image.shape[2] == 4):\n",
    "            image = image[:,:,0:3]\n",
    "        \n",
    "        label= self.labels.iloc[idx, 1]\n",
    "    \n",
    "        #self.sample = ['image', 'labels']\n",
    "        if label==0:\n",
    "            label_=[1,0]\n",
    "        else:\n",
    "            label_=[0,1]\n",
    "        sample = {'image': image, 'labels': label_}\n",
    "        \n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Constructing the dataset\n",
    "cat_dog_dataset = CatandDog(csv_file='train.csv',\n",
    "                                      root_dir='/data/train')\n",
    "\n",
    "# print some stats about the dataset\n",
    "\n",
    "\n",
    "train_set, val_set = torch.utils.data.random_split(cat_dog_dataset, [20000, 4999]) # splitting training data into training, test, cross validation\n",
    "\n",
    "train_loader = DataLoader(val_set, \n",
    "                          batch_size=32,\n",
    "                          shuffle=True, \n",
    "                          num_workers=4)\n",
    "print('Length of dataset: ', len(train_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing some images from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_dog_dataset=CatandDog(csv_file='train.csv',\n",
    "                                      root_dir='data/train')\n",
    "# rand_i = np.random.randint(0, len(cat_dog))\n",
    "# sample = cat_dog[rand_i]\n",
    "#print( sample['image'].shape, sample['labels'].shape)\n",
    "#sample['image'].shape\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    # define the size of images\n",
    "    fig = plt.figure(figsize=(20,10))\n",
    "    \n",
    "    # randomly select a sample\n",
    "    rand_i = np.random.randint(0, len(cat_dog_dataset))\n",
    "    sample = cat_dog_dataset[rand_i]\n",
    "\n",
    "    # print the shape of the image and keypoints\n",
    "    print(i, sample['image'].shape, sample['labels'])\n",
    "    plt.imshow(sample['image'])\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<item>\\Normalize: to convert a color image to grayscale values with a range of [0,1] and normalize the keypoints to be in a range of about [-1, 1]\n",
    "<item>Rescale: to rescale an image to a desired size.\n",
    "<item>RandomCrop: to crop an image randomly.\n",
    "<item>ToTensor: to convert numpy images to torch images.\n",
    "   <p> We  wrote them as callable classes instead of simple functions so that parameters of the transform need not be passed everytime it's called. For this, we just need to implement __call__ method and (if we require parameters to be passed in), the __init__ method. We can then use a transform like this:\n",
    "\n",
    "tx = Transform(params)\n",
    "transformed_sample = tx(sample)\\<p>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms, utils\n",
    "# tranforms\n",
    "\n",
    "class Normalize(object):\n",
    "    \"\"\"Convert a color image to grayscale and normalize the color range to [0,1].\"\"\"        \n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image = sample['image']\n",
    "        label = sample['labels']\n",
    "        image_copy = np.copy(image)\n",
    "        \n",
    "        # convert image to grayscale\n",
    "#         image_copy = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "        \n",
    "        # scale color range from [0, 255] to [0, 1]\n",
    "        image_copy=  image_copy/255.0\n",
    "\n",
    "#         print(image_copy.shape)\n",
    "        return {'image': image_copy, 'labels': label}\n",
    "\n",
    "\n",
    "class Rescale(object):\n",
    "    \"\"\"Rescale the image in a sample to a given size.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image = sample['image']\n",
    "        label = sample['labels']\n",
    "        h, w = image.shape[:2]\n",
    "        if isinstance(self.output_size, int):  # to check if the output_size is of int type\n",
    "            if h > w:\n",
    "                new_h, new_w = self.output_size * h / w, self.output_size\n",
    "            else:\n",
    "                new_h, new_w = self.output_size, self.output_size * w / h\n",
    "        else:\n",
    "            new_h, new_w = self.output_size\n",
    "\n",
    "        new_h, new_w = int(new_h), int(new_w)\n",
    "\n",
    "        img = cv2.resize(image, (new_w, new_h))\n",
    "        \n",
    "       \n",
    "\n",
    "        return {'image': img, 'labels': label} \n",
    "\n",
    "\n",
    "class RandomCrop(object):\n",
    "    \"\"\"Crop randomly the image in a sample.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If int, square crop\n",
    "            is made.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        if isinstance(output_size, int):\n",
    "            self.output_size = (output_size, output_size)\n",
    "        else:\n",
    "            assert len(output_size) == 2\n",
    "            self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image = sample['image']\n",
    "        label = sample['labels']\n",
    "        h, w = image.shape[0:2]\n",
    "        new_h, new_w = self.output_size\n",
    "\n",
    "        top = np.random.randint(0, h - new_h)\n",
    "        left = np.random.randint(0, w - new_w)\n",
    "\n",
    "        image = image[top: top + new_h,\n",
    "                      left: left + new_w]\n",
    "\n",
    "        return {'image': image, 'labels': label}\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image = sample['image']\n",
    "        label = sample['labels']\n",
    "        # if image has no grayscale color channel, add one\n",
    "        if(len(image.shape) == 2):\n",
    "            # add that third color dim\n",
    "            image = image.reshape(image.shape[0], image.shape[1], 1)\n",
    "            \n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        image = image.transpose((2, 0, 1))\n",
    "        return {'image': torch.from_numpy(image), 'labels':label}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing operations on some samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rescale = Rescale(100)\n",
    "crop = RandomCrop(50)\n",
    "composed = transforms.Compose([Rescale(250),\n",
    "                               RandomCrop(224)])\n",
    "\n",
    "# apply the transforms to a sample image\n",
    "test_num = 1000\n",
    "sample = cat_dog_dataset[test_num]\n",
    "\n",
    "fig = plt.figure()\n",
    "for i, tx in enumerate([rescale, crop, composed]):\n",
    "    transformed_sample = tx(sample)\n",
    "\n",
    "    ax = plt.subplot(1, 3, i + 1)\n",
    "    print(transformed_sample['image'].shape, sample['labels'])\n",
    "    plt.imshow(sample['image'])\n",
    "#     plt.tight_layout()\n",
    "    ax.set_title(type(tx).__name__)\n",
    "    plt.imshow(transformed_sample['image'])#\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# order matters! i.e. rescaling should come before a smaller crop\n",
    "data_transform = transforms.Compose([Rescale(250),\n",
    "                                     RandomCrop(224),\n",
    "                                     Normalize(),\n",
    "                                     ToTensor()])\n",
    "\n",
    "# create the transformed dataset\n",
    "transformed_dataset = CatandDog(csv_file='train.csv',\n",
    "                                      root_dir='data/train',transform=data_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print some stats about the transformed data\n",
    "print('Number of images: ', len(transformed_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    sample = transformed_dataset[i]\n",
    "    print(i, sample['image'].size(), sample['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net=Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path= 'model_4_val(1)'\n",
    "model = Net()\n",
    "model.load_state_dict(torch.load(path, map_location=torch.device('cpu')))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model):\n",
    "    net.eval()\n",
    "    for i, (data) in enumerate(train_loader):\n",
    "        images = data['image']\n",
    "        # print(labels)\n",
    "        \n",
    "        images = images.type(torch.FloatTensor)\n",
    "        # #transferring into cuda\n",
    "        images=images.to(device)\n",
    "        # print(images.shape)\n",
    "\n",
    "\n",
    "        # Predict classes using images from the test set\n",
    "        outputs = model(images)\n",
    "        _, prediction = torch.max(outputs.data, 1)\n",
    "        print(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.load_state_dict(path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".pytorch",
   "language": "python",
   "name": ".pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
